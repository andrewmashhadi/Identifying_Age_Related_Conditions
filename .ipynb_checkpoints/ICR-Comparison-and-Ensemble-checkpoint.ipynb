{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83699d31",
   "metadata": {},
   "source": [
    "# ICR - Identifying Age-Related Conditions\n",
    "## Using Machine Learning to detect conditions with measurements of anonymous characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c0391",
   "metadata": {},
   "source": [
    "In this notebook, we evaluate the validation results from the trained models: \n",
    "\n",
    "* **TabTransformer** w/ SMOTE\n",
    "* **SVM** w/ SMOTE\n",
    "* **XGBoost** w/ SMOTE\n",
    "\n",
    "We first load in the validation probability estimates from each model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbbe781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load validation results\n",
    "TABTR_PROBS = pd.read_csv('val_pred_probs/amzn-tab-trans.csv').to_numpy()\n",
    "SVM_PROBS = pd.read_csv('val_pred_probs/svm-tuned.csv').to_numpy()\n",
    "XGB_PROBS = pd.read_csv('val_pred_probs/xgboost-tuned.csv').to_numpy()\n",
    "\n",
    "\n",
    "# include paths to data from local storage location\n",
    "TRAIN_DATA = os.environ['DATAFILES_PATH'] + '/ICR_Competition/' + 'train.csv'\n",
    "\n",
    "# load training data\n",
    "train_df = pd.read_csv(TRAIN_DATA)\n",
    "\n",
    "# allocate\n",
    "X = train_df.drop(columns=['Class', 'Id'])\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "y = train_df['Class'].astype(int)\n",
    "\n",
    "# train-validation split \n",
    "_, _, _, y_val = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257aba8",
   "metadata": {},
   "source": [
    "Define Balanced Logarithmic Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bafbe217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bal_log_loss(p, y):\n",
    "    ind0 = np.where(y==0)[0]\n",
    "    ind1 = np.where(y==1)[0]\n",
    "    \n",
    "    N0 = len(ind0)\n",
    "    N1 = len(ind1)\n",
    "    \n",
    "    y0 = (y==0).astype(int)\n",
    "    y1 = y.astype(int)\n",
    "    \n",
    "    return (- np.sum(y0*np.log(p[:, 0]))/N0 - np.sum(y1*np.log(p[:, 1]))/N1) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8242a5",
   "metadata": {},
   "source": [
    "Compare individual validation results from each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b884c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "***************************************************************************\n",
      "Test Accuracy for TabTransformer: 0.9113\n",
      "Testing Balanced Logarithmic Loss for TabTransformer: 0.4894\n",
      "***************************************************************************\n",
      "***************************************************************************\n",
      "Test Accuracy for SVM: 0.9113\n",
      "Testing Balanced Logarithmic Loss for SVM: 0.4894\n",
      "***************************************************************************\n",
      "***************************************************************************\n",
      "Test Accuracy for XGBoost: 0.9677\n",
      "Testing Balanced Logarithmic Loss for XGBoost: 0.2356\n",
      "***************************************************************************\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "acc = np.mean(np.argmax(SVM_PROBS, 1)==y_val)\n",
    "bll = bal_log_loss(SVM_PROBS, y_val)\n",
    "\n",
    "print(75*\"*\")\n",
    "print(75*\"*\")\n",
    "print(f'Test Accuracy for TabTransformer: {acc:.4f}')\n",
    "print(f'Testing Balanced Logarithmic Loss for TabTransformer: {bll:.4f}')\n",
    "print(75*\"*\")\n",
    "\n",
    "acc = np.mean(np.argmax(SVM_PROBS, 1)==y_val)\n",
    "bll = bal_log_loss(SVM_PROBS, y_val)\n",
    "\n",
    "print(75*\"*\")\n",
    "print(f'Test Accuracy for SVM: {acc:.4f}')\n",
    "print(f'Testing Balanced Logarithmic Loss for SVM: {bll:.4f}')\n",
    "print(75*\"*\")\n",
    "\n",
    "acc = np.mean(np.argmax(XGB_PROBS, 1)==y_val)\n",
    "bll = bal_log_loss(XGB_PROBS, y_val)\n",
    "\n",
    "print(75*\"*\")\n",
    "print(f'Test Accuracy for XGBoost: {acc:.4f}')\n",
    "print(f'Testing Balanced Logarithmic Loss for XGBoost: {bll:.4f}')\n",
    "print(75*\"*\")\n",
    "print(75*\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0667a1",
   "metadata": {},
   "source": [
    " Try combining the output probabilites to create the *ensemble* model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc41c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## try averaging\n",
    "\n",
    "## try logistic regression\n",
    "\n",
    "## compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa20b1",
   "metadata": {},
   "source": [
    "### Output Probabilties \n",
    "\n",
    "We first generate probabilities for the *official* validaiton set with tuned hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2aedb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "train_df = pd.read_csv(TRAIN_DATA)\n",
    "\n",
    "# allocate\n",
    "X = train_df.drop(columns=['Class', 'Id'])\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "y = train_df['Class'].astype(int)\n",
    "\n",
    "# train-validation split \n",
    "X_train_raw, X_val, y_train_raw, y_val = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "X_val['EJ_B'].fillna(value=X_train_raw['EJ_B'].mode())\n",
    "X_val = X_val.fillna(value=X_train_raw.mean())\n",
    "\n",
    "X_train_raw['EJ_B'].fillna(value=X_train_raw['EJ_B'].mode())\n",
    "X_train_raw = X_train_raw.fillna(value=X_train_raw.mean())\n",
    "\n",
    "# over sample the diagnosed patients in training set\n",
    "oversample = SMOTE(random_state=77, sampling_strategy='minority')\n",
    "X_train, y_train = oversample.fit_resample(X_train_raw, y_train_raw)\n",
    "\n",
    "# shuffle (in case the model choice may be impacted by ordering)\n",
    "shuff_ind = np.random.choice(len(y_train), len(y_train), replace=False)\n",
    "\n",
    "X_train = X_train.iloc[shuff_ind,]\n",
    "y_train = y_train.iloc[shuff_ind,]\n",
    "\n",
    "# make uints just in case \n",
    "y_train.astype('uint8')\n",
    "y_val.astype('uint8')\n",
    "\n",
    "# fit TabPFN with optimal hyperparams \n",
    "classifier = make_pipeline(StandardScaler(), SVC(gamma='auto', \n",
    "                                                 kernel='rbf', \n",
    "                                                 C=c, \n",
    "                                                 probability=True))\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# collect val probabilities \n",
    "probs = classifier.predict_proba(X_val.dropna())\n",
    "\n",
    "# store the validation-set predictions in csv format, locally.\n",
    "pd.DataFrame(probs).to_csv(\"val_pred_probs/svm-tuned.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c92665",
   "metadata": {},
   "source": [
    "Then we generate the probabilites for the test set (for submission)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22e722d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit TabPFN with optimal hyperparams \n",
    "classifier = make_pipeline(StandardScaler(), SVC(gamma='auto', \n",
    "                                                 kernel='rbf', \n",
    "                                                 C=c, \n",
    "                                                 probability=True))\n",
    "\n",
    "# fill NaNs\n",
    "X['EJ_B'].fillna(value=X['EJ_B'].mode())\n",
    "X = X.fillna(value=X.mean())\n",
    "\n",
    "# over sample the diagnosed patients in training set\n",
    "oversample = SMOTE(random_state=77, sampling_strategy='minority')\n",
    "X_rs, y_rs = oversample.fit_resample(X, y)\n",
    "\n",
    "# shuffle (in case the model choice may be impacted by ordering)\n",
    "shuff_ind = np.random.choice(len(y_rs), len(y_rs), replace=False)\n",
    "X_rs = X_rs.iloc[shuff_ind,]\n",
    "y_rs = y_rs.iloc[shuff_ind,]\n",
    "\n",
    "# fit xgboost model\n",
    "classifier.fit(X_rs, y_rs)\n",
    "\n",
    "# load testing data\n",
    "test_df = pd.read_csv(TEST_DATA)\n",
    "test_df['EJ_B'] = (test_df['EJ'] == 'B').astype('int')\n",
    "X_test = test_df.drop(columns=['Id', 'EJ'])\n",
    "\n",
    "# collect val probabilities \n",
    "probs = classifier.predict_proba(X_test)\n",
    "\n",
    "# store the test-set predictions in csv format, locally.\n",
    "pd.DataFrame(probs).to_csv(\"test_pred_probs/svm-tuned.csv\", header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
