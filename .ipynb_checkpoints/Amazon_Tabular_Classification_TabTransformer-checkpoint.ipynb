{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d232297",
   "metadata": {},
   "source": [
    "# Tabular classification with Amazon SageMaker TabTransformer algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf3a271",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ccbac955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sagemaker, boto3, json\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd51216b",
   "metadata": {},
   "source": [
    "## First, we store the training and validation data in S3 as instructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "aeea8598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include all paths to data from local storage location\n",
    "TRAIN_DATA = os.environ['DATAFILES_PATH'] + '/ICR_Competition/' + 'train.csv'\n",
    "TEST_DATA = os.environ['DATAFILES_PATH'] + '/ICR_Competition/' + 'test.csv'\n",
    "GREEKS_DATA = os.environ['DATAFILES_PATH'] + '/ICR_Competition/' + 'greeks.csv'\n",
    "\n",
    "# load training data\n",
    "train_df = pd.read_csv(TRAIN_DATA)\n",
    "\n",
    "# allocate\n",
    "X = train_df.drop(columns=['Class', 'Id'])\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "y = train_df['Class'].astype(int)\n",
    "\n",
    "# train-validation split \n",
    "X_train_raw, X_val, y_train_raw, y_val = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# over sample the diagnosed patients in training set\n",
    "ros = RandomOverSampler(random_state=77, sampling_strategy='minority')\n",
    "X_train, y_train = ros.fit_resample(X_train_raw, y_train_raw)\n",
    "\n",
    "# shuffle (in case the model choice may be impacted by ordering)\n",
    "shuff_ind = np.random.choice(len(y_train), len(y_train), replace=False)\n",
    "\n",
    "X_train = X_train.iloc[shuff_ind,]\n",
    "y_train = y_train.iloc[shuff_ind,]\n",
    "\n",
    "# make uints just in case \n",
    "y_train.astype('uint8')\n",
    "y_val.astype('uint8')\n",
    "\n",
    "# create df for train and val as instructed by aws\n",
    "df_train = pd.concat([y_train, X_train], axis=1)\n",
    "numericalVar = df_train.columns.drop(['EJ_B'])]\n",
    "df_train[numericalVar] = df_train[numericalVar].fillna(df_train[numericalVar].mean())\n",
    "df_train = df_train.dropna()\n",
    "\n",
    "df_val = pd.concat([y_val, X_val], axis=1)\n",
    "df_val[numericalVar] = df_val[numericalVar].fillna(df_train[numericalVar].mean())\n",
    "df_val = df_val.dropna()\n",
    "\n",
    "if not os.path.exists('train'):\n",
    "    os.mkdir('train')\n",
    "\n",
    "df_train.to_csv('train/data.csv', index=False, header=False)\n",
    "\n",
    "if not os.path.exists('validation'):\n",
    "    os.mkdir('validation')\n",
    "\n",
    "df_val.to_csv('validation/data.csv', index=False, header=False)\n",
    "\n",
    "\n",
    "# Upload the files to s3\n",
    "s3 = boto3.resource('s3')\n",
    "response = s3.meta.client.upload_file('train/data.csv', \n",
    "                                      'mypersonalprojectdata', \n",
    "                                      'ICR-Data/train/data.csv')\n",
    "\n",
    "response = s3.meta.client.upload_file('validation/data.csv', \n",
    "                                      'mypersonalprojectdata', \n",
    "                                      'ICR-Data/validation/data.csv')\n",
    "\n",
    "\n",
    "# remove files and directories locally\n",
    "os.remove('train/data.csv')\n",
    "os.remove('validation/data.csv')\n",
    "os.rmdir('train')\n",
    "os.rmdir('validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89f05d4",
   "metadata": {},
   "source": [
    "## Next, we set up SageMaker training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ade4907f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session(boto3.session.Session(region_name='us-west-1'))\n",
    "aws_region = boto3.Session().region_name\n",
    "aws_role = 'arn:aws:iam::378421839225:role/service-role/AmazonSageMaker-ExecutionRole-20230623T185897'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "09049994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "\n",
    "train_model_id, train_model_version, train_scope = (\n",
    "    \"pytorch-tabtransformerclassification-model\",\n",
    "    \"*\",\n",
    "    \"training\",\n",
    ")\n",
    "training_instance_type = \"ml.m5.2xlarge\"\n",
    "\n",
    "# Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=aws_region,\n",
    "    framework=None,\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    image_scope=train_scope,\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the training script\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, script_scope=train_scope\n",
    ")\n",
    "# Retrieve the pre-trained model tarball to further fine-tune. In tabular case, however, the pre-trained model tarball is dummy and fine-tune means training from scratch.\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, model_scope=train_scope\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f47078a",
   "metadata": {},
   "source": [
    "### 2.2. Set Training Parameters\n",
    "\n",
    "---\n",
    "\n",
    "Now that we are done with all the setup that is needed, we are ready to train our tabular algorithm. To begin, let us create a [``sageMaker.estimator.Estimator``](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) object. This estimator will launch the training job. \n",
    "\n",
    "There are two kinds of parameters that need to be set for training. The first one are the parameters for the training job. These include: (i) Training data path. This is S3 folder in which the input data is stored, (ii) Output path: This the s3 folder in which the training output is stored. (iii) Training instance type: This indicates the type of machine on which to run the training.\n",
    "\n",
    "The second set of parameters are algorithm specific training hyper-parameters. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3a60a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training data is available in this bucket\n",
    "data_bucket = \"mypersonalprojectdata\"\n",
    "training_data_prefix = \"ICR-Data/train\"\n",
    "validation_data_prefix = \"ICR-Data/validation\"\n",
    "\n",
    "training_dataset_s3_path = f\"s3://{data_bucket}/{training_data_prefix}\"\n",
    "validation_dataset_s3_path = f\"s3://{data_bucket}/{validation_data_prefix}\"\n",
    "\n",
    "\n",
    "output_bucket = sess.default_bucket()\n",
    "output_prefix = \"tabular-training\"\n",
    "\n",
    "s3_output_location = f\"s3://{output_bucket}/{output_prefix}/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba07adda",
   "metadata": {},
   "source": [
    "---\n",
    "For algorithm specific hyper-parameters, we start by fetching python dictionary of the training hyper-parameters that the algorithm accepts with their default values. This can then be overridden to custom values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e426b631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_epochs': '80', 'patience': '10', 'learning_rate': '0.001', 'batch_size': '256', 'input_dim': '32', 'n_blocks': '4', 'attn_dropout': '0.2', 'mlp_dropout': '0.1', 'frac_shared_embed': '0.25'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=train_model_id, model_version=train_model_version\n",
    ")\n",
    "\n",
    "# [Optional] Override default hyperparameters with custom values\n",
    "hyperparameters[\"n_epochs\"] = \"80\"\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1fe56c",
   "metadata": {},
   "source": [
    "# We move on to train with automatic model tuning  \n",
    "\n",
    "We use a HyperparameterTuner object to interact with Amazon SageMaker hyperparameter tuning APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "71a58757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter, CategoricalParameter, HyperparameterTuner\n",
    "\n",
    "use_amt = False\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"learning_rate\": ContinuousParameter(0.001, 0.01, scaling_type=\"Auto\"),\n",
    "    \"batch_size\": CategoricalParameter([128, 256, 512]),\n",
    "    \"attn_dropout\": ContinuousParameter(0.0, 0.8, scaling_type=\"Auto\"),\n",
    "    \"mlp_dropout\": ContinuousParameter(0.0, 0.8, scaling_type=\"Auto\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00983ba",
   "metadata": {},
   "source": [
    "---\n",
    "We start by creating the estimator object with all the required assets and then launch the training job.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e3f3d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "training_job_name = name_from_base(f\"jumpstart-{train_model_id}-training\")\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "tabular_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=s3_output_location,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "69bb3c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: jumpstart-pytorch-tabtransformerclassif-2023-06-29-05-31-46-494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-29 05:31:52 Starting - Starting the training job...\n",
      "2023-06-29 05:32:06 Starting - Preparing the instances for training......\n",
      "2023-06-29 05:33:03 Downloading - Downloading input data......\n",
      "2023-06-29 05:34:01 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-06-29 05:34:12,575 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-06-29 05:34:12,577 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-06-29 05:34:12,585 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-06-29 05:34:12,587 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-06-29 05:34:14,321 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/aiosignal/aiosignal-1.2.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/blis/blis-0.7.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/catalogue/catalogue-2.0.7-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/cramjam/cramjam-2.5.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/cymem/cymem-2.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/distlib/distlib-0.3.4-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/einops/einops-0.4.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fastparquet/fastparquet-0.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/filelock/filelock-3.7.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/frozenlist/frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/gensim/gensim-4.2.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/grpcio/grpcio-1.43.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/importlib_resources/importlib_resources-5.7.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/imutils/imutils-0.5.4.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/langcodes/langcodes-3.3.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/msgpack/msgpack-1.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/murmurhash/murmurhash-1.0.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/numpy/numpy-1.22.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/opencv_contrib_python/opencv_contrib_python-4.5.2.54-cp38-cp38-manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathy/pathy-0.6.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/platformdirs/platformdirs-2.5.2-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/preshed/preshed-3.0.6-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pydantic/pydantic-1.8.2-cp38-cp38-manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyDeprecate/pyDeprecate-0.3.2-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pytorch_widedeep/pytorch_widedeep-1.1.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/ray/ray-1.12.0-cp38-cp38-manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/smart_open/smart_open-5.2.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/spacy/spacy-3.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/spacy_legacy/spacy_legacy-3.0.9-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/spacy_loggers/spacy_loggers-1.0.2-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/srsly/srsly-2.4.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tensorboardX/tensorboardX-2.5-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/thinc/thinc-8.0.15-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torchmetrics/torchmetrics-0.8.2-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/typer/typer-0.4.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/virtualenv/virtualenv-20.14.1-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/wasabi/wasabi-0.9.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/wrapt/wrapt-1.14.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.0.1-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.8/site-packages (from fastparquet==0.8.1->-r requirements.txt (line 8)) (2021.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from fastparquet==0.8.1->-r requirements.txt (line 8)) (1.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.8/site-packages (from gensim==4.2.0->-r requirements.txt (line 11)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.8/site-packages (from grpcio==1.43.0->-r requirements.txt (line 12)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources==5.7.1->-r requirements.txt (line 13)) (3.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from pydantic==1.8.2->-r requirements.txt (line 23)) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow in /opt/conda/lib/python3.8/site-packages (from pytorch-widedeep==1.1.1->-r requirements.txt (line 25)) (5.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from pytorch-widedeep==1.1.1->-r requirements.txt (line 25)) (1.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from pytorch-widedeep==1.1.1->-r requirements.txt (line 25)) (0.24.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from pytorch-widedeep==1.1.1->-r requirements.txt (line 25)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from pytorch-widedeep==1.1.1->-r requirements.txt (line 25)) (4.62.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from ray==1.12.0->-r requirements.txt (line 26)) (2.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.15.3 in /opt/conda/lib/python3.8/site-packages (from ray==1.12.0->-r requirements.txt (line 26)) (3.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from ray==1.12.0->-r requirements.txt (line 26)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.8/site-packages (from ray==1.12.0->-r requirements.txt (line 26)) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs in /opt/conda/lib/python3.8/site-packages (from ray==1.12.0->-r requirements.txt (line 26)) (21.2.0)\u001b[0m\n",
      "\u001b[34mCollecting jsonschema\u001b[0m\n",
      "\u001b[34mDownloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from spacy==3.3.0->-r requirements.txt (line 28)) (3.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from spacy==3.3.0->-r requirements.txt (line 28)) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from spacy==3.3.0->-r requirements.txt (line 28)) (58.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->spacy==3.3.0->-r requirements.txt (line 28)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.1.0->fastparquet==0.8.1->-r requirements.txt (line 8)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.1.0->fastparquet==0.8.1->-r requirements.txt (line 8)) (2021.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->ray==1.12.0->-r requirements.txt (line 26)) (2021.10.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->ray==1.12.0->-r requirements.txt (line 26)) (1.26.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->ray==1.12.0->-r requirements.txt (line 26)) (3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->ray==1.12.0->-r requirements.txt (line 26)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->spacy==3.3.0->-r requirements.txt (line 28)) (2.0.1)\u001b[0m\n",
      "\u001b[34mCollecting pkgutil-resolve-name>=1.3.10\u001b[0m\n",
      "\u001b[34mDownloading pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\u001b[0m\n",
      "\u001b[34mDownloading pyrsistent-0.19.3-py3-none-any.whl (57 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tabulate in /opt/conda/lib/python3.8/site-packages (from ray==1.12.0->-r requirements.txt (line 26)) (0.8.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->pytorch-widedeep==1.1.1->-r requirements.txt (line 25)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->pytorch-widedeep==1.1.1->-r requirements.txt (line 25)) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision->pytorch-widedeep==1.1.1->-r requirements.txt (line 25)) (8.3.2)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: imutils\u001b[0m\n",
      "\u001b[34mBuilding wheel for imutils (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for imutils (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25860 sha256=24d8fb51b96b8ba05c4ed9ed60239cece6a06e63cd4c88b7e37b707bab8b5491\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/fd/e4/e9/4c884662e099e733d692dbe899b279f593a41497be3c212c1d\u001b[0m\n",
      "\u001b[34mSuccessfully built imutils\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pyrsistent, platformdirs, pkgutil-resolve-name, numpy, murmurhash, importlib-resources, frozenlist, filelock, distlib, cymem, catalogue, wasabi, virtualenv, typer, srsly, smart-open, pydantic, preshed, msgpack, jsonschema, grpcio, blis, aiosignal, thinc, tensorboardX, spacy-loggers, spacy-legacy, ray, pyDeprecate, pathy, langcodes, cramjam, wrapt, torchmetrics, spacy, opencv-contrib-python, imutils, gensim, fastparquet, einops, sagemaker-jumpstart-script-utilities, pytorch-widedeep\u001b[0m\n",
      "\u001b[34mAttempting uninstall: numpy\u001b[0m\n",
      "\u001b[34mFound existing installation: numpy 1.19.1\u001b[0m\n",
      "\u001b[34mUninstalling numpy-1.19.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled numpy-1.19.1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mSuccessfully installed aiosignal-1.2.0 blis-0.7.7 catalogue-2.0.7 cramjam-2.5.0 cymem-2.0.6 distlib-0.3.4 einops-0.4.1 fastparquet-0.8.1 filelock-3.7.0 frozenlist-1.3.0 gensim-4.2.0 grpcio-1.43.0 importlib-resources-5.7.1 imutils-0.5.4 jsonschema-4.17.3 langcodes-3.3.0 msgpack-1.0.3 murmurhash-1.0.7 numpy-1.22.3 opencv-contrib-python-4.5.2.54 pathy-0.6.1 pkgutil-resolve-name-1.3.10 platformdirs-2.5.2 preshed-3.0.6 pyDeprecate-0.3.2 pydantic-1.8.2 pyrsistent-0.19.3 pytorch-widedeep-1.1.1 ray-1.12.0 sagemaker-jumpstart-script-utilities-1.0.1 smart-open-5.2.1 spacy-3.3.0 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 tensorboardX-2.5 thinc-8.0.15 torchmetrics-0.8.2 typer-0.4.1 virtualenv-20.14.1 wasabi-0.9.1 wrapt-1.14.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-06-29 05:34:26,531 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-06-29 05:34:26,541 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-06-29 05:34:26,552 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-06-29 05:34:26,560 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"training\": \"/opt/ml/input/data/training\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"attn_dropout\": \"0.2\",\n",
      "        \"batch_size\": \"256\",\n",
      "        \"frac_shared_embed\": \"0.25\",\n",
      "        \"input_dim\": \"32\",\n",
      "        \"learning_rate\": \"0.001\",\n",
      "        \"mlp_dropout\": \"0.1\",\n",
      "        \"n_blocks\": \"4\",\n",
      "        \"n_epochs\": \"80\",\n",
      "        \"patience\": \"10\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"jumpstart-pytorch-tabtransformerclassif-2023-06-29-05-31-46-494\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://jumpstart-cache-prod-us-west-1/source-directory-tarballs/pytorch/transfer_learning/tabtransformerclassification/v1.0.4/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"attn_dropout\":\"0.2\",\"batch_size\":\"256\",\"frac_shared_embed\":\"0.25\",\"input_dim\":\"32\",\"learning_rate\":\"0.001\",\"mlp_dropout\":\"0.1\",\"n_blocks\":\"4\",\"n_epochs\":\"80\",\"patience\":\"10\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model\",\"training\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://jumpstart-cache-prod-us-west-1/source-directory-tarballs/pytorch/transfer_learning/tabtransformerclassification/v1.0.4/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"training\":\"/opt/ml/input/data/training\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"attn_dropout\":\"0.2\",\"batch_size\":\"256\",\"frac_shared_embed\":\"0.25\",\"input_dim\":\"32\",\"learning_rate\":\"0.001\",\"mlp_dropout\":\"0.1\",\"n_blocks\":\"4\",\"n_epochs\":\"80\",\"patience\":\"10\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"jumpstart-pytorch-tabtransformerclassif-2023-06-29-05-31-46-494\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://jumpstart-cache-prod-us-west-1/source-directory-tarballs/pytorch/transfer_learning/tabtransformerclassification/v1.0.4/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--attn_dropout\",\"0.2\",\"--batch_size\",\"256\",\"--frac_shared_embed\",\"0.25\",\"--input_dim\",\"32\",\"--learning_rate\",\"0.001\",\"--mlp_dropout\",\"0.1\",\"--n_blocks\",\"4\",\"--n_epochs\",\"80\",\"--patience\",\"10\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_ATTN_DROPOUT=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_FRAC_SHARED_EMBED=0.25\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT_DIM=32\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_MLP_DROPOUT=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_N_BLOCKS=4\u001b[0m\n",
      "\u001b[34mSM_HP_N_EPOCHS=80\u001b[0m\n",
      "\u001b[34mSM_HP_PATIENCE=10\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 transfer_learning.py --attn_dropout 0.2 --batch_size 256 --frac_shared_embed 0.25 --input_dim 32 --learning_rate 0.001 --mlp_dropout 0.1 --n_blocks 4 --n_epochs 80 --patience 10\u001b[0m\n",
      "\u001b[34mINFO:root:Data in the validation channel is found. Reading the train and validation data from the training and validation channel, respectively.\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:28.911 algo-1:48 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug/core/tfevent/util.py:29: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\u001b[0m\n",
      "\u001b[34mDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.dtype(np.bool): \"DT_BOOL\",\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.018 algo-1:48 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.019 algo-1:48 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.019 algo-1:48 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.019 algo-1:48 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.019 algo-1:48 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mepoch 1:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.cat_and_cont_embed.cont_embed.weight count_params:1792\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.cat_and_cont_embed.cont_embed.bias count_params:1792\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block0.attn.q_proj.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block0.attn.kv_proj.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block0.attn.out_proj.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block0.ff.w_1.weight count_params:4096\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block0.ff.w_1.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block0.ff.w_2.weight count_params:4096\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block0.ff.w_2.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block0.attn_addnorm.ln.weight count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block0.attn_addnorm.ln.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block0.ff_addnorm.ln.weight count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block0.ff_addnorm.ln.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block1.attn.q_proj.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block1.attn.kv_proj.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block1.attn.out_proj.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block1.ff.w_1.weight count_params:4096\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block1.ff.w_1.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block1.ff.w_2.weight count_params:4096\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block1.ff.w_2.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block1.attn_addnorm.ln.weight count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block1.attn_addnorm.ln.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block1.ff_addnorm.ln.weight count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.118 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block1.ff_addnorm.ln.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block2.attn.q_proj.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block2.attn.kv_proj.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block2.attn.out_proj.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block2.ff.w_1.weight count_params:4096\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block2.ff.w_1.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block2.ff.w_2.weight count_params:4096\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block2.ff.w_2.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block2.attn_addnorm.ln.weight count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block2.attn_addnorm.ln.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block2.ff_addnorm.ln.weight count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block2.ff_addnorm.ln.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block3.attn.q_proj.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block3.attn.kv_proj.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block3.attn.out_proj.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block3.ff.w_1.weight count_params:4096\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block3.ff.w_1.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block3.ff.w_2.weight count_params:4096\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block3.ff.w_2.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block3.attn_addnorm.ln.weight count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block3.attn_addnorm.ln.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block3.ff_addnorm.ln.weight count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_blks.transformer_block3.ff_addnorm.ln.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_mlp.mlp.dense_layer_0.0.weight count_params:12845056\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_mlp.mlp.dense_layer_0.0.bias count_params:7168\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_mlp.mlp.dense_layer_1.0.weight count_params:25690112\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.0.transformer_mlp.mlp.dense_layer_1.0.bias count_params:3584\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.1.weight count_params:3584\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:591] name:deeptabular.1.bias count_params:1\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.119 algo-1:48 INFO hook.py:593] Total Trainable Params: 38603393\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.120 algo-1:48 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2023-06-29 05:34:29.122 algo-1:48 INFO hook.py:488] Hook is writing from the hook with pid: 48\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug/core/tfevent/util.py:56: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  dtype=dtype, tensor_content=nparray_data.tostring(), tensor_shape=tps\u001b[0m\n",
      "\u001b[34mepoch 1:   0%|          | 0/3 [00:00<?, ?it/s, loss=0.693, metrics={'f1': 0.5227}]\u001b[0m\n",
      "\u001b[34mepoch 1:  33%|███▎      | 1/3 [00:00<00:01,  1.03it/s, loss=0.693, metrics={'f1': 0.5227}]\u001b[0m\n",
      "\u001b[34mepoch 1:  33%|███▎      | 1/3 [00:00<00:01,  1.03it/s, loss=0.693, metrics={'f1': 0.5227}]\u001b[0m\n",
      "\u001b[34mepoch 1:  33%|███▎      | 1/3 [00:01<00:01,  1.03it/s, loss=0.909, metrics={'f1': 0.6614}]\u001b[0m\n",
      "\u001b[34mepoch 1:  67%|██████▋   | 2/3 [00:01<00:00,  1.38it/s, loss=0.909, metrics={'f1': 0.6614}]\u001b[0m\n",
      "\u001b[34mepoch 1:  67%|██████▋   | 2/3 [00:01<00:00,  1.38it/s, loss=0.909, metrics={'f1': 0.6614}]\u001b[0m\n",
      "\u001b[34mepoch 1:  67%|██████▋   | 2/3 [00:01<00:00,  1.38it/s, loss=9.87, metrics={'f1': 0.5588}]\u001b[0m\n",
      "\u001b[34mepoch 1: 100%|██████████| 3/3 [00:01<00:00,  1.69it/s, loss=9.87, metrics={'f1': 0.5588}]\u001b[0m\n",
      "\u001b[34mepoch 1: 100%|██████████| 3/3 [00:01<00:00,  1.53it/s, loss=9.87, metrics={'f1': 0.5588}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=4.65, metrics={'f1': 0.0}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s, loss=4.65, metrics={'f1': 0.0}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s, loss=4.65, metrics={'f1': 0.0}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 2:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 2:   0%|          | 0/3 [00:01<?, ?it/s, loss=11.8, metrics={'f1': 0.0}]\u001b[0m\n",
      "\u001b[34mepoch 2:  33%|███▎      | 1/3 [00:01<00:03,  1.62s/it, loss=11.8, metrics={'f1': 0.0}]\u001b[0m\n",
      "\u001b[34mepoch 2:  33%|███▎      | 1/3 [00:01<00:03,  1.62s/it, loss=11.8, metrics={'f1': 0.0}]\u001b[0m\n",
      "\u001b[34mepoch 2:  33%|███▎      | 1/3 [00:02<00:03,  1.62s/it, loss=7.09, metrics={'f1': 0.0}]\u001b[0m\n",
      "\u001b[34mepoch 2:  67%|██████▋   | 2/3 [00:02<00:00,  1.00it/s, loss=7.09, metrics={'f1': 0.0}]\u001b[0m\n",
      "\u001b[34mepoch 2:  67%|██████▋   | 2/3 [00:02<00:00,  1.00it/s, loss=7.09, metrics={'f1': 0.0}]\u001b[0m\n",
      "\u001b[34mepoch 2:  67%|██████▋   | 2/3 [00:02<00:00,  1.00it/s, loss=4.96, metrics={'f1': 0.3754}]\u001b[0m\n",
      "\u001b[34mepoch 2: 100%|██████████| 3/3 [00:02<00:00,  1.33it/s, loss=4.96, metrics={'f1': 0.3754}]\u001b[0m\n",
      "\u001b[34mepoch 2: 100%|██████████| 3/3 [00:02<00:00,  1.13it/s, loss=4.96, metrics={'f1': 0.3754}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=1.12, metrics={'f1': 0.3134}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.08it/s, loss=1.12, metrics={'f1': 0.3134}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.08it/s, loss=1.12, metrics={'f1': 0.3134}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 3:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mepoch 3:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.667, metrics={'f1': 0.677}]\u001b[0m\n",
      "\u001b[34mepoch 3:  33%|███▎      | 1/3 [00:01<00:03,  1.58s/it, loss=0.667, metrics={'f1': 0.677}]\u001b[0m\n",
      "\u001b[34mepoch 3:  33%|███▎      | 1/3 [00:01<00:03,  1.58s/it, loss=0.667, metrics={'f1': 0.677}]\u001b[0m\n",
      "\u001b[34mepoch 3:  33%|███▎      | 1/3 [00:02<00:03,  1.58s/it, loss=0.692, metrics={'f1': 0.6939}]\u001b[0m\n",
      "\u001b[34mepoch 3:  67%|██████▋   | 2/3 [00:02<00:00,  1.01it/s, loss=0.692, metrics={'f1': 0.6939}]\u001b[0m\n",
      "\u001b[34mepoch 3:  67%|██████▋   | 2/3 [00:02<00:00,  1.01it/s, loss=0.692, metrics={'f1': 0.6939}]\u001b[0m\n",
      "\u001b[34mepoch 3:  67%|██████▋   | 2/3 [00:02<00:00,  1.01it/s, loss=0.676, metrics={'f1': 0.6867}]\u001b[0m\n",
      "\u001b[34mepoch 3: 100%|██████████| 3/3 [00:02<00:00,  1.34it/s, loss=0.676, metrics={'f1': 0.6867}]\u001b[0m\n",
      "\u001b[34mepoch 3: 100%|██████████| 3/3 [00:02<00:00,  1.15it/s, loss=0.676, metrics={'f1': 0.6867}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.687, metrics={'f1': 0.4255}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  2.94it/s, loss=0.687, metrics={'f1': 0.4255}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  2.94it/s, loss=0.687, metrics={'f1': 0.4255}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 4:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 4:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.601, metrics={'f1': 0.7552}]\u001b[0m\n",
      "\u001b[34mepoch 4:  33%|███▎      | 1/3 [00:01<00:02,  1.50s/it, loss=0.601, metrics={'f1': 0.7552}]\u001b[0m\n",
      "\u001b[34mepoch 4:  33%|███▎      | 1/3 [00:01<00:02,  1.50s/it, loss=0.601, metrics={'f1': 0.7552}]\u001b[0m\n",
      "\u001b[34mepoch 4:  33%|███▎      | 1/3 [00:02<00:02,  1.50s/it, loss=0.59, metrics={'f1': 0.7919}]\u001b[0m\n",
      "\u001b[34mepoch 4:  67%|██████▋   | 2/3 [00:02<00:00,  1.04it/s, loss=0.59, metrics={'f1': 0.7919}]\u001b[0m\n",
      "\u001b[34mepoch 4:  67%|██████▋   | 2/3 [00:02<00:00,  1.04it/s, loss=0.59, metrics={'f1': 0.7919}]\u001b[0m\n",
      "\u001b[34mepoch 4:  67%|██████▋   | 2/3 [00:02<00:00,  1.04it/s, loss=0.568, metrics={'f1': 0.7957}]\u001b[0m\n",
      "\u001b[34mepoch 4: 100%|██████████| 3/3 [00:02<00:00,  1.37it/s, loss=0.568, metrics={'f1': 0.7957}]\u001b[0m\n",
      "\u001b[34mepoch 4: 100%|██████████| 3/3 [00:02<00:00,  1.18it/s, loss=0.568, metrics={'f1': 0.7957}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.775, metrics={'f1': 0.4255}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  2.89it/s, loss=0.775, metrics={'f1': 0.4255}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  2.89it/s, loss=0.775, metrics={'f1': 0.4255}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 5:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 5:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.518, metrics={'f1': 0.7619}]\u001b[0m\n",
      "\u001b[34mepoch 5:  33%|███▎      | 1/3 [00:01<00:03,  1.61s/it, loss=0.518, metrics={'f1': 0.7619}]\u001b[0m\n",
      "\u001b[34mepoch 5:  33%|███▎      | 1/3 [00:01<00:03,  1.61s/it, loss=0.518, metrics={'f1': 0.7619}]\u001b[0m\n",
      "\u001b[34mepoch 5:  33%|███▎      | 1/3 [00:02<00:03,  1.61s/it, loss=0.476, metrics={'f1': 0.7993}]\u001b[0m\n",
      "\u001b[34mepoch 5:  67%|██████▋   | 2/3 [00:02<00:01,  1.02s/it, loss=0.476, metrics={'f1': 0.7993}]\u001b[0m\n",
      "\u001b[34mepoch 5:  67%|██████▋   | 2/3 [00:02<00:01,  1.02s/it, loss=0.476, metrics={'f1': 0.7993}]\u001b[0m\n",
      "\u001b[34mepoch 5:  67%|██████▋   | 2/3 [00:02<00:01,  1.02s/it, loss=0.459, metrics={'f1': 0.7941}]\u001b[0m\n",
      "\u001b[34mepoch 5: 100%|██████████| 3/3 [00:02<00:00,  1.32it/s, loss=0.459, metrics={'f1': 0.7941}]\u001b[0m\n",
      "\u001b[34mepoch 5: 100%|██████████| 3/3 [00:02<00:00,  1.13it/s, loss=0.459, metrics={'f1': 0.7941}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.481, metrics={'f1': 0.5283}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.19it/s, loss=0.481, metrics={'f1': 0.5283}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.19it/s, loss=0.481, metrics={'f1': 0.5283}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 6:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 6:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.36, metrics={'f1': 0.8377}]\u001b[0m\n",
      "\u001b[34mepoch 6:  33%|███▎      | 1/3 [00:01<00:03,  1.57s/it, loss=0.36, metrics={'f1': 0.8377}]\u001b[0m\n",
      "\u001b[34mepoch 6:  33%|███▎      | 1/3 [00:01<00:03,  1.57s/it, loss=0.36, metrics={'f1': 0.8377}]\u001b[0m\n",
      "\u001b[34mepoch 6:  33%|███▎      | 1/3 [00:02<00:03,  1.57s/it, loss=0.35, metrics={'f1': 0.8439}]\u001b[0m\n",
      "\u001b[34mepoch 6:  67%|██████▋   | 2/3 [00:02<00:00,  1.03it/s, loss=0.35, metrics={'f1': 0.8439}]\u001b[0m\n",
      "\u001b[34mepoch 6:  67%|██████▋   | 2/3 [00:02<00:00,  1.03it/s, loss=0.35, metrics={'f1': 0.8439}]\u001b[0m\n",
      "\u001b[34mepoch 6:  67%|██████▋   | 2/3 [00:02<00:00,  1.03it/s, loss=0.357, metrics={'f1': 0.845}]\u001b[0m\n",
      "\u001b[34mepoch 6: 100%|██████████| 3/3 [00:02<00:00,  1.36it/s, loss=0.357, metrics={'f1': 0.845}]\u001b[0m\n",
      "\u001b[34mepoch 6: 100%|██████████| 3/3 [00:02<00:00,  1.16it/s, loss=0.357, metrics={'f1': 0.845}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.374, metrics={'f1': 0.5581}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.38it/s, loss=0.374, metrics={'f1': 0.5581}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.38it/s, loss=0.374, metrics={'f1': 0.5581}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 7:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 7:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.322, metrics={'f1': 0.8571}]\u001b[0m\n",
      "\u001b[34mepoch 7:  33%|███▎      | 1/3 [00:01<00:03,  1.60s/it, loss=0.322, metrics={'f1': 0.8571}]\u001b[0m\n",
      "\u001b[34mepoch 7:  33%|███▎      | 1/3 [00:01<00:03,  1.60s/it, loss=0.322, metrics={'f1': 0.8571}]\u001b[0m\n",
      "\u001b[34mepoch 7:  33%|███▎      | 1/3 [00:02<00:03,  1.60s/it, loss=0.308, metrics={'f1': 0.8779}]\u001b[0m\n",
      "\u001b[34mepoch 7:  67%|██████▋   | 2/3 [00:02<00:01,  1.04s/it, loss=0.308, metrics={'f1': 0.8779}]\u001b[0m\n",
      "\u001b[34mepoch 7:  67%|██████▋   | 2/3 [00:02<00:01,  1.04s/it, loss=0.308, metrics={'f1': 0.8779}]\u001b[0m\n",
      "\u001b[34mepoch 7:  67%|██████▋   | 2/3 [00:02<00:01,  1.04s/it, loss=0.349, metrics={'f1': 0.8688}]\u001b[0m\n",
      "\u001b[34mepoch 7: 100%|██████████| 3/3 [00:02<00:00,  1.28it/s, loss=0.349, metrics={'f1': 0.8688}]\u001b[0m\n",
      "\u001b[34mepoch 7: 100%|██████████| 3/3 [00:02<00:00,  1.11it/s, loss=0.349, metrics={'f1': 0.8688}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.287, metrics={'f1': 0.5405}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.36it/s, loss=0.287, metrics={'f1': 0.5405}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.36it/s, loss=0.287, metrics={'f1': 0.5405}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 8:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 8:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.306, metrics={'f1': 0.8548}]\u001b[0m\n",
      "\u001b[34mepoch 8:  33%|███▎      | 1/3 [00:01<00:03,  1.59s/it, loss=0.306, metrics={'f1': 0.8548}]\u001b[0m\n",
      "\u001b[34mepoch 8:  33%|███▎      | 1/3 [00:01<00:03,  1.59s/it, loss=0.306, metrics={'f1': 0.8548}]\u001b[0m\n",
      "\u001b[34mepoch 8:  33%|███▎      | 1/3 [00:02<00:03,  1.59s/it, loss=0.301, metrics={'f1': 0.8612}]\u001b[0m\n",
      "\u001b[34mepoch 8:  67%|██████▋   | 2/3 [00:02<00:01,  1.04s/it, loss=0.301, metrics={'f1': 0.8612}]\u001b[0m\n",
      "\u001b[34mepoch 8:  67%|██████▋   | 2/3 [00:02<00:01,  1.04s/it, loss=0.301, metrics={'f1': 0.8612}]\u001b[0m\n",
      "\u001b[34mepoch 8:  67%|██████▋   | 2/3 [00:02<00:01,  1.04s/it, loss=0.316, metrics={'f1': 0.8643}]\u001b[0m\n",
      "\u001b[34mepoch 8: 100%|██████████| 3/3 [00:02<00:00,  1.29it/s, loss=0.316, metrics={'f1': 0.8643}]\u001b[0m\n",
      "\u001b[34mepoch 8: 100%|██████████| 3/3 [00:02<00:00,  1.11it/s, loss=0.316, metrics={'f1': 0.8643}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.397, metrics={'f1': 0.7119}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s, loss=0.397, metrics={'f1': 0.7119}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s, loss=0.397, metrics={'f1': 0.7119}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 9:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 9:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.298, metrics={'f1': 0.911}]\u001b[0m\n",
      "\u001b[34mepoch 9:  33%|███▎      | 1/3 [00:01<00:03,  1.58s/it, loss=0.298, metrics={'f1': 0.911}]\u001b[0m\n",
      "\u001b[34mepoch 9:  33%|███▎      | 1/3 [00:01<00:03,  1.58s/it, loss=0.298, metrics={'f1': 0.911}]\u001b[0m\n",
      "\u001b[34mepoch 9:  33%|███▎      | 1/3 [00:02<00:03,  1.58s/it, loss=0.33, metrics={'f1': 0.8809}]\u001b[0m\n",
      "\u001b[34mepoch 9:  67%|██████▋   | 2/3 [00:02<00:01,  1.00s/it, loss=0.33, metrics={'f1': 0.8809}]\u001b[0m\n",
      "\u001b[34mepoch 9:  67%|██████▋   | 2/3 [00:02<00:01,  1.00s/it, loss=0.33, metrics={'f1': 0.8809}]\u001b[0m\n",
      "\u001b[34mepoch 9:  67%|██████▋   | 2/3 [00:02<00:01,  1.00s/it, loss=0.317, metrics={'f1': 0.8787}]\u001b[0m\n",
      "\u001b[34mepoch 9: 100%|██████████| 3/3 [00:02<00:00,  1.32it/s, loss=0.317, metrics={'f1': 0.8787}]\u001b[0m\n",
      "\u001b[34mepoch 9: 100%|██████████| 3/3 [00:02<00:00,  1.13it/s, loss=0.317, metrics={'f1': 0.8787}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.466, metrics={'f1': 0.6562}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.28it/s, loss=0.466, metrics={'f1': 0.6562}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.28it/s, loss=0.466, metrics={'f1': 0.6562}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 10:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 10:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.333, metrics={'f1': 0.8858}]\u001b[0m\n",
      "\u001b[34mepoch 10:  33%|███▎      | 1/3 [00:01<00:03,  1.65s/it, loss=0.333, metrics={'f1': 0.8858}]\u001b[0m\n",
      "\u001b[34mepoch 10:  33%|███▎      | 1/3 [00:01<00:03,  1.65s/it, loss=0.333, metrics={'f1': 0.8858}]\u001b[0m\n",
      "\u001b[34mepoch 10:  33%|███▎      | 1/3 [00:02<00:03,  1.65s/it, loss=0.272, metrics={'f1': 0.9116}]\u001b[0m\n",
      "\u001b[34mepoch 10:  67%|██████▋   | 2/3 [00:02<00:01,  1.01s/it, loss=0.272, metrics={'f1': 0.9116}]\u001b[0m\n",
      "\u001b[34mepoch 10:  67%|██████▋   | 2/3 [00:02<00:01,  1.01s/it, loss=0.272, metrics={'f1': 0.9116}]\u001b[0m\n",
      "\u001b[34mepoch 10:  67%|██████▋   | 2/3 [00:02<00:01,  1.01s/it, loss=0.249, metrics={'f1': 0.9113}]\u001b[0m\n",
      "\u001b[34mepoch 10: 100%|██████████| 3/3 [00:02<00:00,  1.32it/s, loss=0.249, metrics={'f1': 0.9113}]\u001b[0m\n",
      "\u001b[34mepoch 10: 100%|██████████| 3/3 [00:02<00:00,  1.12it/s, loss=0.249, metrics={'f1': 0.9113}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.199, metrics={'f1': 0.6471}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s, loss=0.199, metrics={'f1': 0.6471}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s, loss=0.199, metrics={'f1': 0.6471}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 11:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 11:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.223, metrics={'f1': 0.9076}]\u001b[0m\n",
      "\u001b[34mepoch 11:  33%|███▎      | 1/3 [00:01<00:03,  1.62s/it, loss=0.223, metrics={'f1': 0.9076}]\u001b[0m\n",
      "\u001b[34mepoch 11:  33%|███▎      | 1/3 [00:01<00:03,  1.62s/it, loss=0.223, metrics={'f1': 0.9076}]\u001b[0m\n",
      "\u001b[34mepoch 11:  33%|███▎      | 1/3 [00:02<00:03,  1.62s/it, loss=0.201, metrics={'f1': 0.9231}]\u001b[0m\n",
      "\u001b[34mepoch 11:  67%|██████▋   | 2/3 [00:02<00:01,  1.02s/it, loss=0.201, metrics={'f1': 0.9231}]\u001b[0m\n",
      "\u001b[34mepoch 11:  67%|██████▋   | 2/3 [00:02<00:01,  1.02s/it, loss=0.201, metrics={'f1': 0.9231}]\u001b[0m\n",
      "\u001b[34mepoch 11:  67%|██████▋   | 2/3 [00:02<00:01,  1.02s/it, loss=0.203, metrics={'f1': 0.9277}]\u001b[0m\n",
      "\u001b[34mepoch 11: 100%|██████████| 3/3 [00:02<00:00,  1.26it/s, loss=0.203, metrics={'f1': 0.9277}]\u001b[0m\n",
      "\u001b[34mepoch 11: 100%|██████████| 3/3 [00:02<00:00,  1.09it/s, loss=0.203, metrics={'f1': 0.9277}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.314, metrics={'f1': 0.75}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  2.87it/s, loss=0.314, metrics={'f1': 0.75}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  2.86it/s, loss=0.314, metrics={'f1': 0.75}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 12:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 12:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.258, metrics={'f1': 0.9181}]\u001b[0m\n",
      "\u001b[34mepoch 12:  33%|███▎      | 1/3 [00:01<00:03,  1.77s/it, loss=0.258, metrics={'f1': 0.9181}]\u001b[0m\n",
      "\u001b[34mepoch 12:  33%|███▎      | 1/3 [00:01<00:03,  1.77s/it, loss=0.258, metrics={'f1': 0.9181}]\u001b[0m\n",
      "\u001b[34mepoch 12:  33%|███▎      | 1/3 [00:02<00:03,  1.77s/it, loss=0.202, metrics={'f1': 0.9378}]\u001b[0m\n",
      "\u001b[34mepoch 12:  67%|██████▋   | 2/3 [00:02<00:01,  1.08s/it, loss=0.202, metrics={'f1': 0.9378}]\u001b[0m\n",
      "\u001b[34mepoch 12:  67%|██████▋   | 2/3 [00:02<00:01,  1.08s/it, loss=0.202, metrics={'f1': 0.9378}]\u001b[0m\n",
      "\u001b[34mepoch 12:  67%|██████▋   | 2/3 [00:02<00:01,  1.08s/it, loss=0.204, metrics={'f1': 0.9298}]\u001b[0m\n",
      "\u001b[34mepoch 12: 100%|██████████| 3/3 [00:02<00:00,  1.26it/s, loss=0.204, metrics={'f1': 0.9298}]\u001b[0m\n",
      "\u001b[34mepoch 12: 100%|██████████| 3/3 [00:02<00:00,  1.06it/s, loss=0.204, metrics={'f1': 0.9298}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.179, metrics={'f1': 0.8}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s, loss=0.179, metrics={'f1': 0.8}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s, loss=0.179, metrics={'f1': 0.8}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 13:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 13:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.184, metrics={'f1': 0.9255}]\u001b[0m\n",
      "\u001b[34mepoch 13:  33%|███▎      | 1/3 [00:01<00:03,  1.66s/it, loss=0.184, metrics={'f1': 0.9255}]\u001b[0m\n",
      "\u001b[34mepoch 13:  33%|███▎      | 1/3 [00:01<00:03,  1.66s/it, loss=0.184, metrics={'f1': 0.9255}]\u001b[0m\n",
      "\u001b[34mepoch 13:  33%|███▎      | 1/3 [00:02<00:03,  1.66s/it, loss=0.152, metrics={'f1': 0.9466}]\u001b[0m\n",
      "\u001b[34mepoch 13:  67%|██████▋   | 2/3 [00:02<00:01,  1.04s/it, loss=0.152, metrics={'f1': 0.9466}]\u001b[0m\n",
      "\u001b[34mepoch 13:  67%|██████▋   | 2/3 [00:02<00:01,  1.04s/it, loss=0.152, metrics={'f1': 0.9466}]\u001b[0m\n",
      "\u001b[34mepoch 13:  67%|██████▋   | 2/3 [00:02<00:01,  1.04s/it, loss=0.16, metrics={'f1': 0.9478}]\u001b[0m\n",
      "\u001b[34mepoch 13: 100%|██████████| 3/3 [00:02<00:00,  1.30it/s, loss=0.16, metrics={'f1': 0.9478}]\u001b[0m\n",
      "\u001b[34mepoch 13: 100%|██████████| 3/3 [00:02<00:00,  1.11it/s, loss=0.16, metrics={'f1': 0.9478}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.198, metrics={'f1': 0.8333}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.25it/s, loss=0.198, metrics={'f1': 0.8333}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.25it/s, loss=0.198, metrics={'f1': 0.8333}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 14:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 14:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.167, metrics={'f1': 0.9485}]\u001b[0m\n",
      "\u001b[34mepoch 14:  33%|███▎      | 1/3 [00:01<00:03,  1.57s/it, loss=0.167, metrics={'f1': 0.9485}]\u001b[0m\n",
      "\u001b[34mepoch 14:  33%|███▎      | 1/3 [00:01<00:03,  1.57s/it, loss=0.167, metrics={'f1': 0.9485}]\u001b[0m\n",
      "\u001b[34mepoch 14:  33%|███▎      | 1/3 [00:02<00:03,  1.57s/it, loss=0.144, metrics={'f1': 0.9578}]\u001b[0m\n",
      "\u001b[34mepoch 14:  67%|██████▋   | 2/3 [00:02<00:01,  1.02s/it, loss=0.144, metrics={'f1': 0.9578}]\u001b[0m\n",
      "\u001b[34mepoch 14:  67%|██████▋   | 2/3 [00:02<00:01,  1.02s/it, loss=0.144, metrics={'f1': 0.9578}]\u001b[0m\n",
      "\u001b[34mepoch 14:  67%|██████▋   | 2/3 [00:02<00:01,  1.02s/it, loss=0.141, metrics={'f1': 0.9532}]\u001b[0m\n",
      "\u001b[34mepoch 14: 100%|██████████| 3/3 [00:02<00:00,  1.31it/s, loss=0.141, metrics={'f1': 0.9532}]\u001b[0m\n",
      "\u001b[34mepoch 14: 100%|██████████| 3/3 [00:02<00:00,  1.13it/s, loss=0.141, metrics={'f1': 0.9532}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.169, metrics={'f1': 0.8837}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.21it/s, loss=0.169, metrics={'f1': 0.8837}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.20it/s, loss=0.169, metrics={'f1': 0.8837}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 15:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 15:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.15, metrics={'f1': 0.9585}]\u001b[0m\n",
      "\u001b[34mepoch 15:  33%|███▎      | 1/3 [00:01<00:03,  1.62s/it, loss=0.15, metrics={'f1': 0.9585}]\u001b[0m\n",
      "\u001b[34mepoch 15:  33%|███▎      | 1/3 [00:01<00:03,  1.62s/it, loss=0.15, metrics={'f1': 0.9585}]\u001b[0m\n",
      "\u001b[34mepoch 15:  33%|███▎      | 1/3 [00:02<00:03,  1.62s/it, loss=0.118, metrics={'f1': 0.9708}]\u001b[0m\n",
      "\u001b[34mepoch 15:  67%|██████▋   | 2/3 [00:02<00:01,  1.03s/it, loss=0.118, metrics={'f1': 0.9708}]\u001b[0m\n",
      "\u001b[34mepoch 15:  67%|██████▋   | 2/3 [00:02<00:01,  1.03s/it, loss=0.118, metrics={'f1': 0.9708}]\u001b[0m\n",
      "\u001b[34mepoch 15:  67%|██████▋   | 2/3 [00:02<00:01,  1.03s/it, loss=0.12, metrics={'f1': 0.9707}]\u001b[0m\n",
      "\u001b[34mepoch 15: 100%|██████████| 3/3 [00:02<00:00,  1.29it/s, loss=0.12, metrics={'f1': 0.9707}]\u001b[0m\n",
      "\u001b[34mepoch 15: 100%|██████████| 3/3 [00:02<00:00,  1.11it/s, loss=0.12, metrics={'f1': 0.9707}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.154, metrics={'f1': 0.8571}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s, loss=0.154, metrics={'f1': 0.8571}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s, loss=0.154, metrics={'f1': 0.8571}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 16:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mepoch 16:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.124, metrics={'f1': 0.9736}]\u001b[0m\n",
      "\u001b[34mepoch 16:  33%|███▎      | 1/3 [00:01<00:03,  1.56s/it, loss=0.124, metrics={'f1': 0.9736}]\u001b[0m\n",
      "\u001b[34mepoch 16:  33%|███▎      | 1/3 [00:01<00:03,  1.56s/it, loss=0.124, metrics={'f1': 0.9736}]\u001b[0m\n",
      "\u001b[34mepoch 16:  33%|███▎      | 1/3 [00:02<00:03,  1.56s/it, loss=0.122, metrics={'f1': 0.9607}]\u001b[0m\n",
      "\u001b[34mepoch 16:  67%|██████▋   | 2/3 [00:02<00:01,  1.03s/it, loss=0.122, metrics={'f1': 0.9607}]\u001b[0m\n",
      "\u001b[34mepoch 16:  67%|██████▋   | 2/3 [00:02<00:01,  1.03s/it, loss=0.122, metrics={'f1': 0.9607}]\u001b[0m\n",
      "\u001b[34mepoch 16:  67%|██████▋   | 2/3 [00:02<00:01,  1.03s/it, loss=0.121, metrics={'f1': 0.9597}]\u001b[0m\n",
      "\u001b[34mepoch 16: 100%|██████████| 3/3 [00:02<00:00,  1.28it/s, loss=0.121, metrics={'f1': 0.9597}]\u001b[0m\n",
      "\u001b[34mepoch 16: 100%|██████████| 3/3 [00:02<00:00,  1.11it/s, loss=0.121, metrics={'f1': 0.9597}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.174, metrics={'f1': 0.8511}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.29it/s, loss=0.174, metrics={'f1': 0.8511}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.29it/s, loss=0.174, metrics={'f1': 0.8511}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 17:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 17:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.144, metrics={'f1': 0.9627}]\u001b[0m\n",
      "\u001b[34mepoch 17:  33%|███▎      | 1/3 [00:01<00:03,  1.63s/it, loss=0.144, metrics={'f1': 0.9627}]\u001b[0m\n",
      "\u001b[34mepoch 17:  33%|███▎      | 1/3 [00:01<00:03,  1.63s/it, loss=0.144, metrics={'f1': 0.9627}]\u001b[0m\n",
      "\u001b[34mepoch 17:  33%|███▎      | 1/3 [00:02<00:03,  1.63s/it, loss=0.105, metrics={'f1': 0.9763}]\u001b[0m\n",
      "\u001b[34mepoch 17:  67%|██████▋   | 2/3 [00:02<00:01,  1.04s/it, loss=0.105, metrics={'f1': 0.9763}]\u001b[0m\n",
      "\u001b[34mepoch 17:  67%|██████▋   | 2/3 [00:02<00:01,  1.04s/it, loss=0.105, metrics={'f1': 0.9763}]\u001b[0m\n",
      "\u001b[34mepoch 17:  67%|██████▋   | 2/3 [00:02<00:01,  1.04s/it, loss=0.0992, metrics={'f1': 0.9757}]\u001b[0m\n",
      "\u001b[34mepoch 17: 100%|██████████| 3/3 [00:02<00:00,  1.35it/s, loss=0.0992, metrics={'f1': 0.9757}]\u001b[0m\n",
      "\u001b[34mepoch 17: 100%|██████████| 3/3 [00:02<00:00,  1.13it/s, loss=0.0992, metrics={'f1': 0.9757}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.146, metrics={'f1': 0.8205}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s, loss=0.146, metrics={'f1': 0.8205}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s, loss=0.146, metrics={'f1': 0.8205}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 18:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 18:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.108, metrics={'f1': 0.9695}]\u001b[0m\n",
      "\u001b[34mepoch 18:  33%|███▎      | 1/3 [00:01<00:03,  1.59s/it, loss=0.108, metrics={'f1': 0.9695}]\u001b[0m\n",
      "\u001b[34mepoch 18:  33%|███▎      | 1/3 [00:01<00:03,  1.59s/it, loss=0.108, metrics={'f1': 0.9695}]\u001b[0m\n",
      "\u001b[34mepoch 18:  33%|███▎      | 1/3 [00:02<00:03,  1.59s/it, loss=0.0883, metrics={'f1': 0.9741}]\u001b[0m\n",
      "\u001b[34mepoch 18:  67%|██████▋   | 2/3 [00:02<00:01,  1.05s/it, loss=0.0883, metrics={'f1': 0.9741}]\u001b[0m\n",
      "\u001b[34mepoch 18:  67%|██████▋   | 2/3 [00:02<00:01,  1.05s/it, loss=0.0883, metrics={'f1': 0.9741}]\u001b[0m\n",
      "\u001b[34mepoch 18:  67%|██████▋   | 2/3 [00:02<00:01,  1.05s/it, loss=0.0877, metrics={'f1': 0.9716}]\u001b[0m\n",
      "\u001b[34mepoch 18: 100%|██████████| 3/3 [00:02<00:00,  1.28it/s, loss=0.0877, metrics={'f1': 0.9716}]\u001b[0m\n",
      "\u001b[34mepoch 18: 100%|██████████| 3/3 [00:02<00:00,  1.10it/s, loss=0.0877, metrics={'f1': 0.9716}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.151, metrics={'f1': 0.8182}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.13it/s, loss=0.151, metrics={'f1': 0.8182}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.13it/s, loss=0.151, metrics={'f1': 0.8182}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 19:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 19:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.105, metrics={'f1': 0.9699}]\u001b[0m\n",
      "\u001b[34mepoch 19:  33%|███▎      | 1/3 [00:01<00:03,  1.58s/it, loss=0.105, metrics={'f1': 0.9699}]\u001b[0m\n",
      "\u001b[34mepoch 19:  33%|███▎      | 1/3 [00:01<00:03,  1.58s/it, loss=0.105, metrics={'f1': 0.9699}]\u001b[0m\n",
      "\u001b[34mepoch 19:  33%|███▎      | 1/3 [00:02<00:03,  1.58s/it, loss=0.0838, metrics={'f1': 0.9779}]\u001b[0m\n",
      "\u001b[34mepoch 19:  67%|██████▋   | 2/3 [00:02<00:01,  1.01s/it, loss=0.0838, metrics={'f1': 0.9779}]\u001b[0m\n",
      "\u001b[34mepoch 19:  67%|██████▋   | 2/3 [00:02<00:01,  1.01s/it, loss=0.0838, metrics={'f1': 0.9779}]\u001b[0m\n",
      "\u001b[34mepoch 19:  67%|██████▋   | 2/3 [00:02<00:01,  1.01s/it, loss=0.0739, metrics={'f1': 0.9806}]\u001b[0m\n",
      "\u001b[34mepoch 19: 100%|██████████| 3/3 [00:02<00:00,  1.32it/s, loss=0.0739, metrics={'f1': 0.9806}]\u001b[0m\n",
      "\u001b[34mepoch 19: 100%|██████████| 3/3 [00:02<00:00,  1.14it/s, loss=0.0739, metrics={'f1': 0.9806}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.149, metrics={'f1': 0.8205}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.13it/s, loss=0.149, metrics={'f1': 0.8205}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.13it/s, loss=0.149, metrics={'f1': 0.8205}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 20:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 20:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.0775, metrics={'f1': 0.9773}]\u001b[0m\n",
      "\u001b[34mepoch 20:  33%|███▎      | 1/3 [00:01<00:03,  1.62s/it, loss=0.0775, metrics={'f1': 0.9773}]\u001b[0m\n",
      "\u001b[34mepoch 20:  33%|███▎      | 1/3 [00:01<00:03,  1.62s/it, loss=0.0775, metrics={'f1': 0.9773}]\u001b[0m\n",
      "\u001b[34mepoch 20:  33%|███▎      | 1/3 [00:02<00:03,  1.62s/it, loss=0.0602, metrics={'f1': 0.9815}]\u001b[0m\n",
      "\u001b[34mepoch 20:  67%|██████▋   | 2/3 [00:02<00:01,  1.00s/it, loss=0.0602, metrics={'f1': 0.9815}]\u001b[0m\n",
      "\u001b[34mepoch 20:  67%|██████▋   | 2/3 [00:02<00:01,  1.00s/it, loss=0.0602, metrics={'f1': 0.9815}]\u001b[0m\n",
      "\u001b[34mepoch 20:  67%|██████▋   | 2/3 [00:02<00:01,  1.00s/it, loss=0.0564, metrics={'f1': 0.9832}]\u001b[0m\n",
      "\u001b[34mepoch 20: 100%|██████████| 3/3 [00:02<00:00,  1.33it/s, loss=0.0564, metrics={'f1': 0.9832}]\u001b[0m\n",
      "\u001b[34mepoch 20: 100%|██████████| 3/3 [00:02<00:00,  1.13it/s, loss=0.0564, metrics={'f1': 0.9832}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.151, metrics={'f1': 0.8372}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.26it/s, loss=0.151, metrics={'f1': 0.8372}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.26it/s, loss=0.151, metrics={'f1': 0.8372}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 21:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 21:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.078, metrics={'f1': 0.9699}]\u001b[0m\n",
      "\u001b[34mepoch 21:  33%|███▎      | 1/3 [00:01<00:03,  1.55s/it, loss=0.078, metrics={'f1': 0.9699}]\u001b[0m\n",
      "\u001b[34mepoch 21:  33%|███▎      | 1/3 [00:01<00:03,  1.55s/it, loss=0.078, metrics={'f1': 0.9699}]\u001b[0m\n",
      "\u001b[34mepoch 21:  33%|███▎      | 1/3 [00:02<00:03,  1.55s/it, loss=0.0558, metrics={'f1': 0.9835}]\u001b[0m\n",
      "\u001b[34mepoch 21:  67%|██████▋   | 2/3 [00:02<00:01,  1.03s/it, loss=0.0558, metrics={'f1': 0.9835}]\u001b[0m\n",
      "\u001b[34mepoch 21:  67%|██████▋   | 2/3 [00:02<00:01,  1.03s/it, loss=0.0558, metrics={'f1': 0.9835}]\u001b[0m\n",
      "\u001b[34mepoch 21:  67%|██████▋   | 2/3 [00:02<00:01,  1.03s/it, loss=0.0514, metrics={'f1': 0.9871}]\u001b[0m\n",
      "\u001b[34mepoch 21: 100%|██████████| 3/3 [00:02<00:00,  1.30it/s, loss=0.0514, metrics={'f1': 0.9871}]\u001b[0m\n",
      "\u001b[34mepoch 21: 100%|██████████| 3/3 [00:02<00:00,  1.12it/s, loss=0.0514, metrics={'f1': 0.9871}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.154, metrics={'f1': 0.8718}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.12it/s, loss=0.154, metrics={'f1': 0.8718}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.12it/s, loss=0.154, metrics={'f1': 0.8718}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 22:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 22:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.0516, metrics={'f1': 0.9885}]\u001b[0m\n",
      "\u001b[34mepoch 22:  33%|███▎      | 1/3 [00:01<00:03,  1.60s/it, loss=0.0516, metrics={'f1': 0.9885}]\u001b[0m\n",
      "\u001b[34mepoch 22:  33%|███▎      | 1/3 [00:01<00:03,  1.60s/it, loss=0.0516, metrics={'f1': 0.9885}]\u001b[0m\n",
      "\u001b[34mepoch 22:  33%|███▎      | 1/3 [00:02<00:03,  1.60s/it, loss=0.0372, metrics={'f1': 0.9926}]\u001b[0m\n",
      "\u001b[34mepoch 22:  67%|██████▋   | 2/3 [00:02<00:01,  1.00s/it, loss=0.0372, metrics={'f1': 0.9926}]\u001b[0m\n",
      "\u001b[34mepoch 22:  67%|██████▋   | 2/3 [00:02<00:01,  1.00s/it, loss=0.0372, metrics={'f1': 0.9926}]\u001b[0m\n",
      "\u001b[34mepoch 22:  67%|██████▋   | 2/3 [00:02<00:01,  1.00s/it, loss=0.0353, metrics={'f1': 0.9922}]\u001b[0m\n",
      "\u001b[34mepoch 22: 100%|██████████| 3/3 [00:02<00:00,  1.33it/s, loss=0.0353, metrics={'f1': 0.9922}]\u001b[0m\n",
      "\u001b[34mepoch 22: 100%|██████████| 3/3 [00:02<00:00,  1.13it/s, loss=0.0353, metrics={'f1': 0.9922}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.167, metrics={'f1': 0.8421}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.28it/s, loss=0.167, metrics={'f1': 0.8421}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.28it/s, loss=0.167, metrics={'f1': 0.8421}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 23:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 23:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.053, metrics={'f1': 0.9771}]\u001b[0m\n",
      "\u001b[34mepoch 23:  33%|███▎      | 1/3 [00:01<00:03,  1.68s/it, loss=0.053, metrics={'f1': 0.9771}]\u001b[0m\n",
      "\u001b[34mepoch 23:  33%|███▎      | 1/3 [00:01<00:03,  1.68s/it, loss=0.053, metrics={'f1': 0.9771}]\u001b[0m\n",
      "\u001b[34mepoch 23:  33%|███▎      | 1/3 [00:02<00:03,  1.68s/it, loss=0.0427, metrics={'f1': 0.9852}]\u001b[0m\n",
      "\u001b[34mepoch 23:  67%|██████▋   | 2/3 [00:02<00:00,  1.00it/s, loss=0.0427, metrics={'f1': 0.9852}]\u001b[0m\n",
      "\u001b[34mepoch 23:  67%|██████▋   | 2/3 [00:02<00:00,  1.00it/s, loss=0.0427, metrics={'f1': 0.9852}]\u001b[0m\n",
      "\u001b[34mepoch 23:  67%|██████▋   | 2/3 [00:02<00:00,  1.00it/s, loss=0.0389, metrics={'f1': 0.9884}]\u001b[0m\n",
      "\u001b[34mepoch 23: 100%|██████████| 3/3 [00:02<00:00,  1.34it/s, loss=0.0389, metrics={'f1': 0.9884}]\u001b[0m\n",
      "\u001b[34mepoch 23: 100%|██████████| 3/3 [00:02<00:00,  1.14it/s, loss=0.0389, metrics={'f1': 0.9884}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.16, metrics={'f1': 0.8293}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s, loss=0.16, metrics={'f1': 0.8293}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s, loss=0.16, metrics={'f1': 0.8293}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 24:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mepoch 24:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.0305, metrics={'f1': 0.9886}]\u001b[0m\n",
      "\u001b[34mepoch 24:  33%|███▎      | 1/3 [00:01<00:03,  1.62s/it, loss=0.0305, metrics={'f1': 0.9886}]\u001b[0m\n",
      "\u001b[34mepoch 24:  33%|███▎      | 1/3 [00:01<00:03,  1.62s/it, loss=0.0305, metrics={'f1': 0.9886}]\u001b[0m\n",
      "\u001b[34mepoch 24:  33%|███▎      | 1/3 [00:02<00:03,  1.62s/it, loss=0.0249, metrics={'f1': 0.9908}]\u001b[0m\n",
      "\u001b[34mepoch 24:  67%|██████▋   | 2/3 [00:02<00:01,  1.03s/it, loss=0.0249, metrics={'f1': 0.9908}]\u001b[0m\n",
      "\u001b[34mepoch 24:  67%|██████▋   | 2/3 [00:02<00:01,  1.03s/it, loss=0.0249, metrics={'f1': 0.9908}]\u001b[0m\n",
      "\u001b[34mepoch 24:  67%|██████▋   | 2/3 [00:02<00:01,  1.03s/it, loss=0.0221, metrics={'f1': 0.9935}]\u001b[0m\n",
      "\u001b[34mepoch 24: 100%|██████████| 3/3 [00:02<00:00,  1.31it/s, loss=0.0221, metrics={'f1': 0.9935}]\u001b[0m\n",
      "\u001b[34mepoch 24: 100%|██████████| 3/3 [00:02<00:00,  1.12it/s, loss=0.0221, metrics={'f1': 0.9935}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.179, metrics={'f1': 0.8205}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.28it/s, loss=0.179, metrics={'f1': 0.8205}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.28it/s, loss=0.179, metrics={'f1': 0.8205}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 25:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 25:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.0162, metrics={'f1': 0.9962}]\u001b[0m\n",
      "\u001b[34mepoch 25:  33%|███▎      | 1/3 [00:01<00:03,  1.58s/it, loss=0.0162, metrics={'f1': 0.9962}]\u001b[0m\n",
      "\u001b[34mepoch 25:  33%|███▎      | 1/3 [00:01<00:03,  1.58s/it, loss=0.0162, metrics={'f1': 0.9962}]\u001b[0m\n",
      "\u001b[34mepoch 25:  33%|███▎      | 1/3 [00:02<00:03,  1.58s/it, loss=0.0169, metrics={'f1': 0.9963}]\u001b[0m\n",
      "\u001b[34mepoch 25:  67%|██████▋   | 2/3 [00:02<00:01,  1.02s/it, loss=0.0169, metrics={'f1': 0.9963}]\u001b[0m\n",
      "\u001b[34mepoch 25:  67%|██████▋   | 2/3 [00:02<00:01,  1.02s/it, loss=0.0169, metrics={'f1': 0.9963}]\u001b[0m\n",
      "\u001b[34mepoch 25:  67%|██████▋   | 2/3 [00:02<00:01,  1.02s/it, loss=0.0168, metrics={'f1': 0.9961}]\u001b[0m\n",
      "\u001b[34mepoch 25: 100%|██████████| 3/3 [00:02<00:00,  1.31it/s, loss=0.0168, metrics={'f1': 0.9961}]\u001b[0m\n",
      "\u001b[34mepoch 25: 100%|██████████| 3/3 [00:02<00:00,  1.13it/s, loss=0.0168, metrics={'f1': 0.9961}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.185, metrics={'f1': 0.85}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s, loss=0.185, metrics={'f1': 0.85}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.36it/s, loss=0.185, metrics={'f1': 0.85}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 26:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 26:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.0112, metrics={'f1': 1.0}]\u001b[0m\n",
      "\u001b[34mepoch 26:  33%|███▎      | 1/3 [00:01<00:03,  1.62s/it, loss=0.0112, metrics={'f1': 1.0}]\u001b[0m\n",
      "\u001b[34mepoch 26:  33%|███▎      | 1/3 [00:01<00:03,  1.62s/it, loss=0.0112, metrics={'f1': 1.0}]\u001b[0m\n",
      "\u001b[34mepoch 26:  33%|███▎      | 1/3 [00:02<00:03,  1.62s/it, loss=0.00914, metrics={'f1': 1.0}]\u001b[0m\n",
      "\u001b[34mepoch 26:  67%|██████▋   | 2/3 [00:02<00:01,  1.02s/it, loss=0.00914, metrics={'f1': 1.0}]\u001b[0m\n",
      "\u001b[34mepoch 26:  67%|██████▋   | 2/3 [00:02<00:01,  1.02s/it, loss=0.00914, metrics={'f1': 1.0}]\u001b[0m\n",
      "\u001b[34mepoch 26:  67%|██████▋   | 2/3 [00:02<00:01,  1.02s/it, loss=0.0122, metrics={'f1': 0.9987}]\u001b[0m\n",
      "\u001b[34mepoch 26: 100%|██████████| 3/3 [00:02<00:00,  1.32it/s, loss=0.0122, metrics={'f1': 0.9987}]\u001b[0m\n",
      "\u001b[34mepoch 26: 100%|██████████| 3/3 [00:02<00:00,  1.13it/s, loss=0.0122, metrics={'f1': 0.9987}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.199, metrics={'f1': 0.7895}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s, loss=0.199, metrics={'f1': 0.7895}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s, loss=0.199, metrics={'f1': 0.7895}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mepoch 27:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\n",
      "2023-06-29 05:36:02 Uploading - Uploading generated training model\u001b[34mepoch 27:   0%|          | 0/3 [00:01<?, ?it/s, loss=0.00732, metrics={'f1': 1.0}]\u001b[0m\n",
      "\u001b[34mepoch 27:  33%|███▎      | 1/3 [00:01<00:03,  1.62s/it, loss=0.00732, metrics={'f1': 1.0}]\u001b[0m\n",
      "\u001b[34mepoch 27:  33%|███▎      | 1/3 [00:01<00:03,  1.62s/it, loss=0.00732, metrics={'f1': 1.0}]\u001b[0m\n",
      "\u001b[34mepoch 27:  33%|███▎      | 1/3 [00:02<00:03,  1.62s/it, loss=0.00748, metrics={'f1': 1.0}]\u001b[0m\n",
      "\u001b[34mepoch 27:  67%|██████▋   | 2/3 [00:02<00:01,  1.04s/it, loss=0.00748, metrics={'f1': 1.0}]\u001b[0m\n",
      "\u001b[34mepoch 27:  67%|██████▋   | 2/3 [00:02<00:01,  1.04s/it, loss=0.00748, metrics={'f1': 1.0}]\u001b[0m\n",
      "\u001b[34mepoch 27:  67%|██████▋   | 2/3 [00:02<00:01,  1.04s/it, loss=0.00745, metrics={'f1': 1.0}]\u001b[0m\n",
      "\u001b[34mepoch 27: 100%|██████████| 3/3 [00:02<00:00,  1.29it/s, loss=0.00745, metrics={'f1': 1.0}]\u001b[0m\n",
      "\u001b[34mepoch 27: 100%|██████████| 3/3 [00:02<00:00,  1.11it/s, loss=0.00745, metrics={'f1': 1.0}]\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mvalid:   0%|          | 0/1 [00:00<?, ?it/s, loss=0.188, metrics={'f1': 0.8293}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.25it/s, loss=0.188, metrics={'f1': 0.8293}]\u001b[0m\n",
      "\u001b[34mvalid: 100%|██████████| 1/1 [00:00<00:00,  3.25it/s, loss=0.188, metrics={'f1': 0.8293}]\u001b[0m\n",
      "\u001b[34mINFO:root:Training is completed.\u001b[0m\n",
      "\u001b[34mINFO:root:Saving model...\u001b[0m\n",
      "\u001b[34m2023-06-29 05:35:53,034 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-06-29 05:36:18 Completed - Training job completed\n",
      "Training seconds: 195\n",
      "Billable seconds: 195\n"
     ]
    }
   ],
   "source": [
    "if use_amt:\n",
    "    tuner = HyperparameterTuner(\n",
    "        tabular_estimator,\n",
    "        \"f1_score\",\n",
    "        hyperparameter_ranges,\n",
    "        [{\"Name\": \"f1_score\", \"Regex\": \"metrics={'f1': (\\\\S+)}\"}],\n",
    "        max_jobs=10,  # increase the max_jobs to achieve better performance from hyperparameter tuning\n",
    "        max_parallel_jobs=2,\n",
    "        objective_type=\"Maximize\",\n",
    "        base_tuning_job_name=training_job_name,\n",
    "    )\n",
    "\n",
    "    tuner.fit({\"training\": training_dataset_s3_path, \"validation\": validation_dataset_s3_path}, logs=True)\n",
    "else:\n",
    "    # Launch a SageMaker Training job by passing s3 path of the training data\n",
    "    tabular_estimator.fit(\n",
    "        {\"training\": training_dataset_s3_path, \n",
    "         \"validation\": validation_dataset_s3_path}, logs=True, job_name=training_job_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb57b46",
   "metadata": {},
   "source": [
    "We now deploy and run inference on the Trained Tabular Model\n",
    "\n",
    "---\n",
    "\n",
    "In this section, you learn how to query an existing endpoint and make predictions of the examples you input. For each example, the model will output the probability of the sample for each class in the model. \n",
    "Next, the predicted class label is obtained by taking the class label with the maximum probability over others. Throughout the notebook, the examples are taken from the [Adult](https://archive.ics.uci.edu/ml/datasets/adult) test set.\n",
    "The dataset contains examples of census data to predict whether a person makes over 50K a year or not.\n",
    "\n",
    "\n",
    "We start by retrieving the jumpstart artifacts and deploy the `tabular_estimator` that we trained.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "08f4cb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-us-west-1-378421839225/tabular-training/output/jumpstart-pytorch-tabtransformerclassif-2023-06-29-05-31-46-494/output/model.tar.gz), script artifact (s3://jumpstart-cache-prod-us-west-1/source-directory-tarballs/pytorch/inference/tabtransformerclassification/v1.0.2/sourcedir.tar.gz), and dependencies ([]) into single tar.gz file located at s3://sagemaker-us-west-1-378421839225/sagemaker-jumpstart-2023-06-29-05-40-49-281/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: sagemaker-jumpstart-2023-06-29-05-40-49-281\n",
      "INFO:sagemaker:Creating endpoint-config with name jumpstart-inference-pytorch-tabtransfor-2023-06-29-05-40-49-281\n",
      "INFO:sagemaker:Creating endpoint with name jumpstart-inference-pytorch-tabtransfor-2023-06-29-05-40-49-281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "inference_instance_type = \"ml.m5.2xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=aws_region,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "# Retrieve the inference script uri\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-inference-{train_model_id}\")\n",
    "\n",
    "# Use the estimator from the previous step to deploy to a SageMaker endpoint\n",
    "predictor = (tuner if use_amt else tabular_estimator).deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    entry_point=\"inference.py\",\n",
    "    image_uri=deploy_image_uri,\n",
    "    source_dir=deploy_source_uri,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "14ed77f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_47</th>\n",
       "      <th>Feature_48</th>\n",
       "      <th>Feature_49</th>\n",
       "      <th>Feature_50</th>\n",
       "      <th>Feature_51</th>\n",
       "      <th>Feature_52</th>\n",
       "      <th>Feature_53</th>\n",
       "      <th>Feature_54</th>\n",
       "      <th>Feature_55</th>\n",
       "      <th>Feature_56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>0.478576</td>\n",
       "      <td>5192.25520</td>\n",
       "      <td>194.576478</td>\n",
       "      <td>13.230384</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>7.290957</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>9.705080</td>\n",
       "      <td>8.588216</td>\n",
       "      <td>...</td>\n",
       "      <td>10.690335</td>\n",
       "      <td>1.85861</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>12.418170</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>434.357883</td>\n",
       "      <td>34.411808</td>\n",
       "      <td>36.769312</td>\n",
       "      <td>0.050038</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>0</td>\n",
       "      <td>1.096024</td>\n",
       "      <td>4348.11080</td>\n",
       "      <td>546.489750</td>\n",
       "      <td>72.469800</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.167092</td>\n",
       "      <td>0.102921</td>\n",
       "      <td>3.396778</td>\n",
       "      <td>13.381312</td>\n",
       "      <td>...</td>\n",
       "      <td>5.760795</td>\n",
       "      <td>2.02884</td>\n",
       "      <td>0.182871</td>\n",
       "      <td>12.283291</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>2184.856740</td>\n",
       "      <td>33.204344</td>\n",
       "      <td>40.169496</td>\n",
       "      <td>0.077344</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>0.307656</td>\n",
       "      <td>3039.47402</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>14.534221</td>\n",
       "      <td>8.715528</td>\n",
       "      <td>5.262246</td>\n",
       "      <td>0.031668</td>\n",
       "      <td>11.665002</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>7.534620</td>\n",
       "      <td>139.519779</td>\n",
       "      <td>10093.114350</td>\n",
       "      <td>30.456385</td>\n",
       "      <td>56.463116</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1</td>\n",
       "      <td>1.307538</td>\n",
       "      <td>8922.64648</td>\n",
       "      <td>198.478110</td>\n",
       "      <td>123.582688</td>\n",
       "      <td>13.380066</td>\n",
       "      <td>9.231078</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>16.404106</td>\n",
       "      <td>28.561792</td>\n",
       "      <td>...</td>\n",
       "      <td>6.015965</td>\n",
       "      <td>0.71224</td>\n",
       "      <td>0.331877</td>\n",
       "      <td>10.101972</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>4973.463261</td>\n",
       "      <td>46.062259</td>\n",
       "      <td>47.428966</td>\n",
       "      <td>0.027720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>0</td>\n",
       "      <td>0.145282</td>\n",
       "      <td>4773.60488</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>8.368094</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.587895</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>8.577022</td>\n",
       "      <td>6.718768</td>\n",
       "      <td>...</td>\n",
       "      <td>3.811895</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>14.669254</td>\n",
       "      <td>188.923924</td>\n",
       "      <td>7886.435724</td>\n",
       "      <td>29.485204</td>\n",
       "      <td>63.986216</td>\n",
       "      <td>0.231559</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0</td>\n",
       "      <td>0.452938</td>\n",
       "      <td>4570.76532</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>15.175300</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>4.296615</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>12.118746</td>\n",
       "      <td>7.042056</td>\n",
       "      <td>...</td>\n",
       "      <td>4.272654</td>\n",
       "      <td>1.59848</td>\n",
       "      <td>0.765349</td>\n",
       "      <td>26.845572</td>\n",
       "      <td>110.058389</td>\n",
       "      <td>18417.619300</td>\n",
       "      <td>26.646081</td>\n",
       "      <td>26.867112</td>\n",
       "      <td>0.242104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.348249</td>\n",
       "      <td>1733.65412</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>8.377385</td>\n",
       "      <td>15.312480</td>\n",
       "      <td>1.913544</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>6.547778</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>...</td>\n",
       "      <td>4.408484</td>\n",
       "      <td>0.86130</td>\n",
       "      <td>0.467337</td>\n",
       "      <td>17.878444</td>\n",
       "      <td>192.453107</td>\n",
       "      <td>3332.467494</td>\n",
       "      <td>34.166222</td>\n",
       "      <td>100.086808</td>\n",
       "      <td>0.065096</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0</td>\n",
       "      <td>0.871692</td>\n",
       "      <td>4085.77675</td>\n",
       "      <td>142.305060</td>\n",
       "      <td>17.107828</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>13.182192</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>15.773906</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>...</td>\n",
       "      <td>4.140112</td>\n",
       "      <td>1.36387</td>\n",
       "      <td>0.501202</td>\n",
       "      <td>34.119736</td>\n",
       "      <td>88.348834</td>\n",
       "      <td>8597.759661</td>\n",
       "      <td>20.811553</td>\n",
       "      <td>29.251356</td>\n",
       "      <td>0.238408</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0</td>\n",
       "      <td>0.495668</td>\n",
       "      <td>3249.88576</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>69.787798</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>5.607747</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>12.540980</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>...</td>\n",
       "      <td>9.199799</td>\n",
       "      <td>1.15188</td>\n",
       "      <td>1.327508</td>\n",
       "      <td>26.054902</td>\n",
       "      <td>98.640444</td>\n",
       "      <td>33885.487350</td>\n",
       "      <td>36.119747</td>\n",
       "      <td>51.496584</td>\n",
       "      <td>0.160135</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1</td>\n",
       "      <td>0.452938</td>\n",
       "      <td>2239.98588</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>47.622569</td>\n",
       "      <td>9.093096</td>\n",
       "      <td>6.077274</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>6.894388</td>\n",
       "      <td>7.031514</td>\n",
       "      <td>...</td>\n",
       "      <td>8.891521</td>\n",
       "      <td>0.86014</td>\n",
       "      <td>0.250601</td>\n",
       "      <td>18.762134</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>15226.211120</td>\n",
       "      <td>23.252529</td>\n",
       "      <td>93.487056</td>\n",
       "      <td>0.108085</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Target  Feature_1   Feature_2   Feature_3   Feature_4  Feature_5  \\\n",
       "49        1   0.478576  5192.25520  194.576478   13.230384   8.138688   \n",
       "581       0   1.096024  4348.11080  546.489750   72.469800   8.138688   \n",
       "82        0   0.307656  3039.47402   85.200147   14.534221   8.715528   \n",
       "109       1   1.307538  8922.64648  198.478110  123.582688  13.380066   \n",
       "605       0   0.145282  4773.60488   85.200147    8.368094   8.138688   \n",
       "..      ...        ...         ...         ...         ...        ...   \n",
       "383       0   0.452938  4570.76532   85.200147   15.175300   8.138688   \n",
       "6         0   0.348249  1733.65412   85.200147    8.377385  15.312480   \n",
       "104       0   0.871692  4085.77675  142.305060   17.107828   8.138688   \n",
       "158       0   0.495668  3249.88576   85.200147   69.787798   8.138688   \n",
       "181       1   0.452938  2239.98588   85.200147   47.622569   9.093096   \n",
       "\n",
       "     Feature_6  Feature_7  Feature_8  Feature_9  ...  Feature_47  Feature_48  \\\n",
       "49    7.290957   0.025578   9.705080   8.588216  ...   10.690335     1.85861   \n",
       "581   3.167092   0.102921   3.396778  13.381312  ...    5.760795     2.02884   \n",
       "82    5.262246   0.031668  11.665002   1.229900  ...    0.173229     0.49706   \n",
       "109   9.231078   0.025578  16.404106  28.561792  ...    6.015965     0.71224   \n",
       "605   3.587895   0.025578   8.577022   6.718768  ...    3.811895     0.49706   \n",
       "..         ...        ...        ...        ...  ...         ...         ...   \n",
       "383   4.296615   0.025578  12.118746   7.042056  ...    4.272654     1.59848   \n",
       "6     1.913544   0.025578   6.547778   1.229900  ...    4.408484     0.86130   \n",
       "104  13.182192   0.025578  15.773906   1.229900  ...    4.140112     1.36387   \n",
       "158   5.607747   0.025578  12.540980   1.229900  ...    9.199799     1.15188   \n",
       "181   6.077274   0.025578   6.894388   7.031514  ...    8.891521     0.86014   \n",
       "\n",
       "     Feature_49  Feature_50  Feature_51    Feature_52  Feature_53  Feature_54  \\\n",
       "49     0.067730   12.418170   72.611063    434.357883   34.411808   36.769312   \n",
       "581    0.182871   12.283291   72.611063   2184.856740   33.204344   40.169496   \n",
       "82     0.067730    7.534620  139.519779  10093.114350   30.456385   56.463116   \n",
       "109    0.331877   10.101972   72.611063   4973.463261   46.062259   47.428966   \n",
       "605    0.067730   14.669254  188.923924   7886.435724   29.485204   63.986216   \n",
       "..          ...         ...         ...           ...         ...         ...   \n",
       "383    0.765349   26.845572  110.058389  18417.619300   26.646081   26.867112   \n",
       "6      0.467337   17.878444  192.453107   3332.467494   34.166222  100.086808   \n",
       "104    0.501202   34.119736   88.348834   8597.759661   20.811553   29.251356   \n",
       "158    1.327508   26.054902   98.640444  33885.487350   36.119747   51.496584   \n",
       "181    0.250601   18.762134   72.611063  15226.211120   23.252529   93.487056   \n",
       "\n",
       "     Feature_55  Feature_56  \n",
       "49     0.050038           1  \n",
       "581    0.077344           1  \n",
       "82    21.978000           0  \n",
       "109    0.027720           1  \n",
       "605    0.231559           1  \n",
       "..          ...         ...  \n",
       "383    0.242104           1  \n",
       "6      0.065096           1  \n",
       "104    0.238408           1  \n",
       "158    0.160135           1  \n",
       "181    0.108085           1  \n",
       "\n",
       "[113 rows x 57 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data\n",
    "test_data = df_val #pd.read_csv(test_data_file_name, header=None)\n",
    "test_data.columns = [\"Target\"] + [f\"Feature_{i}\" for i in range(1, test_data.shape[1])]\n",
    "\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "33f97980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThe test dataset contains 113 examples and 57 columns.\u001b[0m\n",
      "\n",
      "\u001b[1mThe first 5 observations of the data: \u001b[0m \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_47</th>\n",
       "      <th>Feature_48</th>\n",
       "      <th>Feature_49</th>\n",
       "      <th>Feature_50</th>\n",
       "      <th>Feature_51</th>\n",
       "      <th>Feature_52</th>\n",
       "      <th>Feature_53</th>\n",
       "      <th>Feature_54</th>\n",
       "      <th>Feature_55</th>\n",
       "      <th>Feature_56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>0.478576</td>\n",
       "      <td>5192.25520</td>\n",
       "      <td>194.576478</td>\n",
       "      <td>13.230384</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>7.290957</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>9.705080</td>\n",
       "      <td>8.588216</td>\n",
       "      <td>...</td>\n",
       "      <td>10.690335</td>\n",
       "      <td>1.85861</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>12.418170</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>434.357883</td>\n",
       "      <td>34.411808</td>\n",
       "      <td>36.769312</td>\n",
       "      <td>0.050038</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>0</td>\n",
       "      <td>1.096024</td>\n",
       "      <td>4348.11080</td>\n",
       "      <td>546.489750</td>\n",
       "      <td>72.469800</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.167092</td>\n",
       "      <td>0.102921</td>\n",
       "      <td>3.396778</td>\n",
       "      <td>13.381312</td>\n",
       "      <td>...</td>\n",
       "      <td>5.760795</td>\n",
       "      <td>2.02884</td>\n",
       "      <td>0.182871</td>\n",
       "      <td>12.283291</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>2184.856740</td>\n",
       "      <td>33.204344</td>\n",
       "      <td>40.169496</td>\n",
       "      <td>0.077344</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>0.307656</td>\n",
       "      <td>3039.47402</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>14.534221</td>\n",
       "      <td>8.715528</td>\n",
       "      <td>5.262246</td>\n",
       "      <td>0.031668</td>\n",
       "      <td>11.665002</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>7.534620</td>\n",
       "      <td>139.519779</td>\n",
       "      <td>10093.114350</td>\n",
       "      <td>30.456385</td>\n",
       "      <td>56.463116</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1</td>\n",
       "      <td>1.307538</td>\n",
       "      <td>8922.64648</td>\n",
       "      <td>198.478110</td>\n",
       "      <td>123.582688</td>\n",
       "      <td>13.380066</td>\n",
       "      <td>9.231078</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>16.404106</td>\n",
       "      <td>28.561792</td>\n",
       "      <td>...</td>\n",
       "      <td>6.015965</td>\n",
       "      <td>0.71224</td>\n",
       "      <td>0.331877</td>\n",
       "      <td>10.101972</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>4973.463261</td>\n",
       "      <td>46.062259</td>\n",
       "      <td>47.428966</td>\n",
       "      <td>0.027720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>0</td>\n",
       "      <td>0.145282</td>\n",
       "      <td>4773.60488</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>8.368094</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.587895</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>8.577022</td>\n",
       "      <td>6.718768</td>\n",
       "      <td>...</td>\n",
       "      <td>3.811895</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>14.669254</td>\n",
       "      <td>188.923924</td>\n",
       "      <td>7886.435724</td>\n",
       "      <td>29.485204</td>\n",
       "      <td>63.986216</td>\n",
       "      <td>0.231559</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Target  Feature_1   Feature_2   Feature_3   Feature_4  Feature_5  \\\n",
       "49        1   0.478576  5192.25520  194.576478   13.230384   8.138688   \n",
       "581       0   1.096024  4348.11080  546.489750   72.469800   8.138688   \n",
       "82        0   0.307656  3039.47402   85.200147   14.534221   8.715528   \n",
       "109       1   1.307538  8922.64648  198.478110  123.582688  13.380066   \n",
       "605       0   0.145282  4773.60488   85.200147    8.368094   8.138688   \n",
       "\n",
       "     Feature_6  Feature_7  Feature_8  Feature_9  ...  Feature_47  Feature_48  \\\n",
       "49    7.290957   0.025578   9.705080   8.588216  ...   10.690335     1.85861   \n",
       "581   3.167092   0.102921   3.396778  13.381312  ...    5.760795     2.02884   \n",
       "82    5.262246   0.031668  11.665002   1.229900  ...    0.173229     0.49706   \n",
       "109   9.231078   0.025578  16.404106  28.561792  ...    6.015965     0.71224   \n",
       "605   3.587895   0.025578   8.577022   6.718768  ...    3.811895     0.49706   \n",
       "\n",
       "     Feature_49  Feature_50  Feature_51    Feature_52  Feature_53  Feature_54  \\\n",
       "49     0.067730   12.418170   72.611063    434.357883   34.411808   36.769312   \n",
       "581    0.182871   12.283291   72.611063   2184.856740   33.204344   40.169496   \n",
       "82     0.067730    7.534620  139.519779  10093.114350   30.456385   56.463116   \n",
       "109    0.331877   10.101972   72.611063   4973.463261   46.062259   47.428966   \n",
       "605    0.067730   14.669254  188.923924   7886.435724   29.485204   63.986216   \n",
       "\n",
       "     Feature_55  Feature_56  \n",
       "49     0.050038           1  \n",
       "581    0.077344           1  \n",
       "82    21.978000           0  \n",
       "109    0.027720           1  \n",
       "605    0.231559           1  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read the data\n",
    "test_data = df_val #pd.read_csv(test_data_file_name, header=None)\n",
    "test_data.columns = [\"Target\"] + [f\"Feature_{i}\" for i in range(1, test_data.shape[1])]\n",
    "\n",
    "num_examples, num_columns = test_data.shape\n",
    "print(\n",
    "    f\"{bold}The test dataset contains {num_examples} examples and {num_columns} columns.{unbold}\\n\"\n",
    ")\n",
    "\n",
    "# prepare the ground truth target and predicting features to send into the endpoint.\n",
    "ground_truth_label, features = test_data.iloc[:, :1], test_data.iloc[:, 1:]\n",
    "\n",
    "print(f\"{bold}The first 5 observations of the data: {unbold} \\n\")\n",
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a2d1c",
   "metadata": {},
   "source": [
    "---\n",
    "The following code queries the endpoint you have created to get the prediction for each test example. \n",
    "The `query_endpoint()` function returns an array-like of shape (num_examples, num_classes), where each row indicates\n",
    "the probability of the example for each class in the model. The num_classes is 2 in above test data.\n",
    "Next, the predicted class label is obtained by taking the class label with the maximum probability over others for each example. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cbc8fd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_type = \"text/csv\"\n",
    "\n",
    "\n",
    "def query_endpoint(encoded_tabular_data):\n",
    "    # endpoint_name = endpoint_name\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=content_type, Body=encoded_tabular_data\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    predicted_probabilities = model_predictions[\"probabilities\"]\n",
    "    return np.array(predicted_probabilities)\n",
    "\n",
    "\n",
    "# split the test data into smaller size of batches to query the endpoint due to the large size of test data.\n",
    "batch_size = 1500\n",
    "predict_prob = []\n",
    "for i in np.arange(0, num_examples, step=batch_size):\n",
    "    query_response_batch = query_endpoint(\n",
    "        features.iloc[i : (i + batch_size), :].to_csv(header=False, index=False).encode(\"utf-8\")\n",
    "    )\n",
    "    predict_prob_batch = parse_response(query_response_batch)  # prediction probability per batch\n",
    "    predict_prob.append(predict_prob_batch)\n",
    "\n",
    "\n",
    "predict_prob = np.concatenate(predict_prob, axis=0)\n",
    "predict_label = np.argmax(predict_prob, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0224fb1",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Prediction Results Returned from the Endpoint\n",
    "\n",
    "---\n",
    "We evaluate the predictions results returned from the endpoint by following two ways.\n",
    "\n",
    "* Visualize the predictions results by plotting the confusion matrix.\n",
    "\n",
    "* Measure the prediction results quantitatively.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "03d331de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 750x750 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the predictions results by plotting the confusion matrix.\n",
    "conf_matrix = confusion_matrix(y_true=ground_truth_label.values, y_pred=predict_label)\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i, s=conf_matrix[i, j], va=\"center\", ha=\"center\", size=\"xx-large\")\n",
    "\n",
    "plt.xlabel(\"Predictions\", fontsize=18)\n",
    "plt.ylabel(\"Actuals\", fontsize=18)\n",
    "plt.title(\"Confusion Matrix\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9d2d2d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mEvaluation result on test data\u001b[0m:\n",
      "\u001b[1maccuracy_score\u001b[0m: 0.9380530973451328\n",
      "\u001b[1mF1 \u001b[0m: 0.8292682926829269\n",
      "\u001b[1mLog-Loss \u001b[0m: 0.3206098951884387\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def bal_log_loss(p, y):\n",
    "    ind0 = np.where(y==0)[0]\n",
    "    ind1 = np.where(y==1)[0]\n",
    "    \n",
    "    N0 = len(ind0)\n",
    "    N1 = len(ind1)\n",
    "    \n",
    "    y0 = (y==0).astype(int)\n",
    "    y1 = y.astype(int)\n",
    "    \n",
    "    return (- np.sum(y0*np.log(p[:, 0]))/N0 - np.sum(y1*np.log(p[:, 1]))/N1) / 2\n",
    "\n",
    "\n",
    "# Measure the prediction results quantitatively.\n",
    "eval_accuracy = accuracy_score(ground_truth_label.values, predict_label)\n",
    "eval_f1 = f1_score(ground_truth_label.values, predict_label)\n",
    "eval_log_loss = bal_log_loss(predict_prob, np.squeeze(ground_truth_label.values, axis=(1,)))\n",
    "\n",
    "print(\n",
    "    f\"{bold}Evaluation result on test data{unbold}:{newline}\"\n",
    "    f\"{bold}{accuracy_score.__name__}{unbold}: {eval_accuracy}{newline}\"\n",
    "    f\"{bold}F1 {unbold}: {eval_f1}{newline}\"\n",
    "    f\"{bold}Log-Loss {unbold}: {eval_log_loss}{newline}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f57c5",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we delete the endpoint corresponding to the trained model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7ff6901e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: sagemaker-jumpstart-2023-06-29-05-40-49-281\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: jumpstart-inference-pytorch-tabtransfor-2023-06-29-05-40-49-281\n",
      "INFO:sagemaker:Deleting endpoint with name: jumpstart-inference-pytorch-tabtransfor-2023-06-29-05-40-49-281\n"
     ]
    }
   ],
   "source": [
    "# Delete the SageMaker endpoint and the attached resources\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
